{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, shutil, tarfile, glob\n",
    "from functools import partial\n",
    "from contextlib import contextmanager, nullcontext\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf, ListConfig\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchvision.utils import make_grid\n",
    "from einops import rearrange, repeat\n",
    "from pytorch_lightning.utilities.distributed import rank_zero_only\n",
    "\n",
    "import taming.data.utils as tdu\n",
    "from taming.data.imagenet import str_to_indices, give_synsets_from_indices, download, retrieve\n",
    "from ldm.util import log_txt_as_img, exists, ismap, isimage, mean_flat, count_params, instantiate_from_config\n",
    "from ldm.modules.ema import LitEma\n",
    "from ldm.modules.distributions.distributions import normal_kl, DiagonalGaussianDistribution\n",
    "from ldm.models.autoencoder import VQModelInterface, IdentityFirstStage, AutoencoderKL\n",
    "from ldm.modules.diffusionmodules.util import make_beta_schedule, extract_into_tensor, noise_like\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.util import default\n",
    "\n",
    "__conditioning_keys__ = {'concat': 'c_concat',\n",
    "                         'crossattn': 'c_crossattn',\n",
    "                         'adm': 'y'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_1 = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device 1: {device_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disabled_train(self, mode=True):\n",
    "    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n",
    "    does not change anymore.\"\"\"\n",
    "    return self\n",
    "\n",
    "\n",
    "def uniform_on_device(r1, r2, shape, device):\n",
    "    return (r1 - r2) * torch.rand(*shape, device=device) + r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synset2idx(path_to_yaml=\"data/index_synset.yaml\"):\n",
    "    with open(path_to_yaml) as f:\n",
    "        di2s = yaml.load(f)\n",
    "    return dict((v,k) for k,v in di2s.items())\n",
    "\n",
    "class ImageNetBase(Dataset):\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or OmegaConf.create()\n",
    "        if not type(self.config)==dict:\n",
    "            self.config = OmegaConf.to_container(self.config)\n",
    "        self.keep_orig_class_label = self.config.get(\"keep_orig_class_label\", False)\n",
    "        self.process_images = True  # if False we skip loading & processing images and self.data contains filepaths\n",
    "        self._prepare()\n",
    "        self._prepare_synset_to_human()\n",
    "        self._prepare_idx_to_synset()\n",
    "        self._prepare_human_to_integer_label()\n",
    "        self._load()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "    def _prepare(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _filter_relpaths(self, relpaths):\n",
    "        ignore = set([\n",
    "            \"n06596364_9591.JPEG\",\n",
    "        ])\n",
    "        relpaths = [rpath for rpath in relpaths if not rpath.split(\"/\")[-1] in ignore]\n",
    "        if \"sub_indices\" in self.config:\n",
    "            indices = str_to_indices(self.config[\"sub_indices\"])\n",
    "            synsets = give_synsets_from_indices(indices, path_to_yaml=self.idx2syn)  # returns a list of strings\n",
    "            self.synset2idx = synset2idx(path_to_yaml=self.idx2syn)\n",
    "            files = []\n",
    "            for rpath in relpaths:\n",
    "                syn = rpath.split(\"/\")[0]\n",
    "                if syn in synsets:\n",
    "                    files.append(rpath)\n",
    "            return files\n",
    "        else:\n",
    "            return relpaths\n",
    "\n",
    "    def _prepare_synset_to_human(self):\n",
    "        SIZE = 2655750\n",
    "        URL = \"https://heibox.uni-heidelberg.de/f/9f28e956cd304264bb82/?dl=1\"\n",
    "        self.human_dict = os.path.join(self.root, \"synset_human.txt\")\n",
    "        if (not os.path.exists(self.human_dict) or\n",
    "                not os.path.getsize(self.human_dict)==SIZE):\n",
    "            download(URL, self.human_dict)\n",
    "\n",
    "    def _prepare_idx_to_synset(self):\n",
    "        URL = \"https://heibox.uni-heidelberg.de/f/d835d5b6ceda4d3aa910/?dl=1\"\n",
    "        self.idx2syn = os.path.join(self.root, \"index_synset.yaml\")\n",
    "        if (not os.path.exists(self.idx2syn)):\n",
    "            download(URL, self.idx2syn)\n",
    "\n",
    "    def _prepare_human_to_integer_label(self):\n",
    "        URL = \"https://heibox.uni-heidelberg.de/f/2362b797d5be43b883f6/?dl=1\"\n",
    "        self.human2integer = os.path.join(self.root, \"imagenet1000_clsidx_to_labels.txt\")\n",
    "        if (not os.path.exists(self.human2integer)):\n",
    "            download(URL, self.human2integer)\n",
    "        with open(self.human2integer, \"r\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "            assert len(lines) == 1000\n",
    "            self.human2integer_dict = dict()\n",
    "            for line in lines:\n",
    "                value, key = line.split(\":\")\n",
    "                self.human2integer_dict[key] = int(value)\n",
    "\n",
    "    def _load(self):\n",
    "        with open(self.txt_filelist, \"r\") as f:\n",
    "            self.relpaths = f.read().splitlines()\n",
    "            l1 = len(self.relpaths)\n",
    "            self.relpaths = self._filter_relpaths(self.relpaths)\n",
    "            print(\"Removed {} files from filelist during filtering.\".format(l1 - len(self.relpaths)))\n",
    "\n",
    "        self.synsets = [p.split(\"/\")[0] for p in self.relpaths]\n",
    "        self.abspaths = [os.path.join(self.datadir, p) for p in self.relpaths]\n",
    "\n",
    "        unique_synsets = np.unique(self.synsets)\n",
    "        class_dict = dict((synset, i) for i, synset in enumerate(unique_synsets))\n",
    "        if not self.keep_orig_class_label:\n",
    "            self.class_labels = [class_dict[s] for s in self.synsets]\n",
    "        else:\n",
    "            self.class_labels = [self.synset2idx[s] for s in self.synsets]\n",
    "\n",
    "        with open(self.human_dict, \"r\") as f:\n",
    "            human_dict = f.read().splitlines()\n",
    "            human_dict = dict(line.split(maxsplit=1) for line in human_dict)\n",
    "\n",
    "        self.human_labels = [human_dict[s] for s in self.synsets]\n",
    "\n",
    "        labels = {\n",
    "            \"relpath\": np.array(self.relpaths),\n",
    "            \"synsets\": np.array(self.synsets),\n",
    "            \"class_label\": np.array(self.class_labels),\n",
    "            \"human_label\": np.array(self.human_labels),\n",
    "        }\n",
    "\n",
    "        if self.process_images:\n",
    "            self.size = retrieve(self.config, \"size\", default=256)\n",
    "            self.data = ImagePaths(self.abspaths,\n",
    "                                   labels=labels,\n",
    "                                   size=self.size,\n",
    "                                   random_crop=self.random_crop,\n",
    "                                   )\n",
    "        else:\n",
    "            self.data = self.abspaths\n",
    "\n",
    "\n",
    "class ImageNetTrain(ImageNetBase):\n",
    "    NAME = \"train\"\n",
    "    URL = \"http://www.image-net.org/challenges/LSVRC/2012/\"\n",
    "    AT_HASH = \"a306397ccf9c2ead27155983c254227c0fd938e2\"\n",
    "    FILES = [\n",
    "        \"ILSVRC2012_img_train.tar\",\n",
    "    ]\n",
    "    SIZES = [\n",
    "        147897477120,\n",
    "    ]\n",
    "\n",
    "    def __init__(self, process_images=True, data_root=None, **kwargs):\n",
    "        self.process_images = process_images\n",
    "        self.data_root = data_root\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _prepare(self):\n",
    "        if self.data_root:\n",
    "            self.root = os.path.join(self.data_root, self.NAME)\n",
    "        else:\n",
    "            cachedir = os.environ.get(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\"))\n",
    "            self.root = os.path.join(cachedir, \"autoencoders/data\", self.NAME)\n",
    "\n",
    "        self.datadir = os.path.join(self.root)\n",
    "        self.txt_filelist = os.path.join(\"/mnt/sata/train\", \"filelist.txt\")\n",
    "        self.expected_length = 1281167\n",
    "        self.random_crop = retrieve(self.config, \"ImageNetTrain/random_crop\",\n",
    "                                    default=True)\n",
    "        if not tdu.is_prepared(self.root):\n",
    "            # prep\n",
    "            print(\"Preparing dataset {} in {}\".format(self.NAME, self.root))\n",
    "\n",
    "            datadir = self.datadir\n",
    "            if not os.path.exists(datadir):\n",
    "                path = os.path.join(self.root, self.FILES[0])\n",
    "                if not os.path.exists(path) or not os.path.getsize(path)==self.SIZES[0]:\n",
    "                    import academictorrents as at\n",
    "                    atpath = at.get(self.AT_HASH, datastore=self.root)\n",
    "                    assert atpath == path\n",
    "\n",
    "                print(\"Extracting {} to {}\".format(path, datadir))\n",
    "                os.makedirs(datadir, exist_ok=True)\n",
    "                with tarfile.open(path, \"r:\") as tar:\n",
    "                    tar.extractall(path=datadir)\n",
    "\n",
    "                print(\"Extracting sub-tars.\")\n",
    "                subpaths = sorted(glob.glob(os.path.join(datadir, \"*.tar\")))\n",
    "                for subpath in tqdm(subpaths):\n",
    "                    subdir = subpath[:-len(\".tar\")]\n",
    "                    os.makedirs(subdir, exist_ok=True)\n",
    "                    with tarfile.open(subpath, \"r:\") as tar:\n",
    "                        tar.extractall(path=subdir)\n",
    "        datadir = self.datadir\n",
    "        filelist = glob.glob(os.path.join(datadir, \"**\", \"*.JPEG\"))\n",
    "        filelist = [os.path.relpath(p, start=datadir) for p in filelist][:10000]\n",
    "        filelist = sorted(filelist)\n",
    "        filelist = \"\\n\".join(filelist)+\"\\n\"\n",
    "        with open(self.txt_filelist, \"w\") as f:\n",
    "            f.write(filelist)\n",
    "\n",
    "            # tdu.mark_prepared(self.root)\n",
    "\n",
    "\n",
    "class ImageNetValidation(ImageNetBase):\n",
    "    NAME = \"val\"\n",
    "    URL = \"http://www.image-net.org/challenges/LSVRC/2012/\"\n",
    "    AT_HASH = \"5d6d0df7ed81efd49ca99ea4737e0ae5e3a5f2e5\"\n",
    "    VS_URL = \"https://heibox.uni-heidelberg.de/f/3e0f6e9c624e45f2bd73/?dl=1\"\n",
    "    FILES = [\n",
    "        \"ILSVRC2012_img_val.tar\",\n",
    "        \"validation_synset.txt\",\n",
    "    ]\n",
    "    SIZES = [\n",
    "        6744924160,\n",
    "        1950000,\n",
    "    ]\n",
    "\n",
    "    def __init__(self, process_images=True, data_root=None, **kwargs):\n",
    "        self.data_root = data_root\n",
    "        self.process_images = process_images\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _prepare(self):\n",
    "        if self.data_root:\n",
    "            self.root = os.path.join(self.data_root, self.NAME)\n",
    "        else:\n",
    "            cachedir = os.environ.get(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\"))\n",
    "            self.root = os.path.join(cachedir, \"autoencoders/data\", self.NAME)\n",
    "        self.datadir = os.path.join(self.root)\n",
    "        self.txt_filelist = os.path.join(\"/mnt/sata/val\", \"filelist.txt\")\n",
    "        self.expected_length = 50000\n",
    "        self.random_crop = retrieve(self.config, \"ImageNetValidation/random_crop\",\n",
    "                                    default=False)\n",
    "        if not tdu.is_prepared(self.root):\n",
    "            # prep\n",
    "            print(\"Preparing dataset {} in {}\".format(self.NAME, self.root))\n",
    "\n",
    "            if not os.path.exists(datadir):\n",
    "                path = os.path.join(self.root, self.FILES[0])\n",
    "                if not os.path.exists(path) or not os.path.getsize(path)==self.SIZES[0]:\n",
    "                    import academictorrents as at\n",
    "                    atpath = at.get(self.AT_HASH, datastore=self.root)\n",
    "                    assert atpath == path\n",
    "\n",
    "                print(\"Extracting {} to {}\".format(path, datadir))\n",
    "                os.makedirs(datadir, exist_ok=True)\n",
    "                with tarfile.open(path, \"r:\") as tar:\n",
    "                    tar.extractall(path=datadir)\n",
    "\n",
    "                vspath = os.path.join(self.root, self.FILES[1])\n",
    "                if not os.path.exists(vspath) or not os.path.getsize(vspath)==self.SIZES[1]:\n",
    "                    download(self.VS_URL, vspath)\n",
    "\n",
    "                with open(vspath, \"r\") as f:\n",
    "                    synset_dict = f.read().splitlines()\n",
    "                    synset_dict = dict(line.split() for line in synset_dict)\n",
    "\n",
    "                print(\"Reorganizing into synset folders\")\n",
    "                synsets = np.unique(list(synset_dict.values()))\n",
    "                for s in synsets:\n",
    "                    os.makedirs(os.path.join(datadir, s), exist_ok=True)\n",
    "                for k, v in synset_dict.items():\n",
    "                    src = os.path.join(datadir, k)\n",
    "                    dst = os.path.join(datadir, v)\n",
    "                    shutil.move(src, dst)\n",
    "        datadir = self.datadir\n",
    "        filelist = glob.glob(os.path.join(datadir, \"**\", \"*.JPEG\"))\n",
    "        filelist = [os.path.relpath(p, start=datadir) for p in filelist][:50]\n",
    "        filelist = sorted(filelist)\n",
    "        filelist = \"\\n\".join(filelist)+\"\\n\"\n",
    "        with open(self.txt_filelist, \"w\") as f:\n",
    "            f.write(filelist)\n",
    "\n",
    "            # tdu.mark_prepared(self.root)\n",
    "\n",
    "\n",
    "class ImagePaths(Dataset):\n",
    "    def __init__(self, paths, size=None, random_crop=False, labels=None):\n",
    "        self.size = size\n",
    "        self.random_crop = random_crop\n",
    "\n",
    "        self.labels = dict() if labels is None else labels\n",
    "        self.labels[\"file_path_\"] = paths\n",
    "        self._length = len(paths)\n",
    "\n",
    "        if self.size is not None and self.size > 0:\n",
    "            self.rescaler = albumentations.SmallestMaxSize(max_size = self.size)\n",
    "            if not self.random_crop:\n",
    "                self.cropper = albumentations.CenterCrop(height=self.size,width=self.size)\n",
    "            else:\n",
    "                self.cropper = albumentations.RandomCrop(height=self.size,width=self.size)\n",
    "            self.preprocessor = albumentations.Compose([self.rescaler, self.cropper])\n",
    "        else:\n",
    "            self.preprocessor = lambda **kwargs: kwargs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = self.preprocessor(image=image)[\"image\"]\n",
    "        image = (image/127.5 - 1.0).astype(np.float32)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = dict()\n",
    "        example[\"image\"] = self.preprocess_image(self.labels[\"file_path_\"][i])\n",
    "        for k in self.labels:\n",
    "            example[k] = self.labels[k][i]\n",
    "        return example\n",
    "\n",
    "\n",
    "class NumpyPaths(ImagePaths):\n",
    "    def preprocess_image(self, image_path):\n",
    "        image = np.load(image_path).squeeze(0)  # 3 x 1024 x 1024\n",
    "        image = np.transpose(image, (1,2,0))\n",
    "        image = Image.fromarray(image, mode=\"RGB\")\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = self.preprocessor(image=image)[\"image\"]\n",
    "        image = (image/127.5 - 1.0).astype(np.float32)\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dataset = ImageNetTrain(\n",
    "    data_root=r\"/home/hyeonan/imagenet\"\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiffusionWrapper(nn.Module):\n",
    "    def __init__(self, diff_model_config, conditioning_key):\n",
    "        super(DiffusionWrapper, self).__init__()\n",
    "        self.diffusion_model = instantiate_from_config(diff_model_config)\n",
    "        self.conditioning_key = conditioning_key\n",
    "        assert self.conditioning_key in [None, 'concat', 'crossattn', 'hybrid', 'adm', 'hybrid-adm']\n",
    "\n",
    "    def forward(self, x, t, c_concat=None, c_crossattn=None, c_adm=None):\n",
    "        if self.conditioning_key is None:\n",
    "            out = self.diffusion_model(x, t)\n",
    "        elif self.conditioning_key == 'concat':\n",
    "            xc = torch.cat([x] + c_concat, dim=1)\n",
    "            out = self.diffusion_model(xc, t)\n",
    "        elif self.conditioning_key == 'crossattn':\n",
    "            cc = torch.cat(c_crossattn, 1)\n",
    "            out = self.diffusion_model(x, t, context=cc)\n",
    "        elif self.conditioning_key == 'hybrid':\n",
    "            xc = torch.cat([x] + c_concat, dim=1)\n",
    "            cc = torch.cat(c_crossattn, 1)\n",
    "            out = self.diffusion_model(xc, t, context=cc)\n",
    "        elif self.conditioning_key == 'hybrid-adm':\n",
    "            assert c_adm is not None\n",
    "            xc = torch.cat([x] + c_concat, dim=1)\n",
    "            cc = torch.cat(c_crossattn, 1)\n",
    "            out = self.diffusion_model(xc, t, context=cc, y=c_adm)\n",
    "        elif self.conditioning_key == 'adm':\n",
    "            cc = c_crossattn[0]\n",
    "            out = self.diffusion_model(x, t, y=cc)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        return out\n",
    "    \n",
    "class DDPM(nn.Module):\n",
    "    # classic DDPM with Gaussian diffusion, in image space\n",
    "    def __init__(self,\n",
    "                 unet_config,\n",
    "                 timesteps=1000,\n",
    "                 beta_schedule=\"linear\",\n",
    "                 loss_type=\"l2\",\n",
    "                 ckpt_path=None,\n",
    "                 ignore_keys=[],\n",
    "                 load_only_unet=False,\n",
    "                 monitor=\"val/loss\",\n",
    "                 use_ema=True,\n",
    "                 first_stage_key=\"image\",\n",
    "                 image_size=256,\n",
    "                 channels=3,\n",
    "                 log_every_t=100,\n",
    "                 clip_denoised=True,\n",
    "                 linear_start=1e-4,\n",
    "                 linear_end=2e-2,\n",
    "                 cosine_s=8e-3,\n",
    "                 given_betas=None,\n",
    "                 original_elbo_weight=0.,\n",
    "                 v_posterior=0.,  # weight for choosing posterior variance as sigma = (1-v) * beta_tilde + v * beta\n",
    "                 l_simple_weight=1.,\n",
    "                 conditioning_key=None,\n",
    "                 parameterization=\"eps\",  # all assuming fixed variance schedules\n",
    "                 scheduler_config=None,\n",
    "                 use_positional_encodings=False,\n",
    "                 learn_logvar=False,\n",
    "                 logvar_init=0.,\n",
    "                 make_it_fit=False,\n",
    "                 ucg_training=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert parameterization in [\"eps\", \"x0\"], 'currently only supporting \"eps\" and \"x0\"'\n",
    "        self.parameterization = parameterization\n",
    "        print(f\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\")\n",
    "        self.cond_stage_model = None\n",
    "        self.clip_denoised = clip_denoised\n",
    "        self.log_every_t = log_every_t\n",
    "        self.first_stage_key = first_stage_key\n",
    "        self.image_size = image_size  # try conv?\n",
    "        self.channels = channels\n",
    "        self.use_positional_encodings = use_positional_encodings\n",
    "        self.model = DiffusionWrapper(unet_config, conditioning_key)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        count_params(self.model, verbose=True)\n",
    "        self.use_ema = use_ema\n",
    "        if self.use_ema:\n",
    "            self.model_ema = LitEma(self.model)\n",
    "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
    "\n",
    "        self.use_scheduler = scheduler_config is not None\n",
    "        if self.use_scheduler:\n",
    "            self.scheduler_config = scheduler_config\n",
    "\n",
    "        self.v_posterior = v_posterior\n",
    "        self.original_elbo_weight = original_elbo_weight\n",
    "        self.l_simple_weight = l_simple_weight\n",
    "\n",
    "        if monitor is not None:\n",
    "            self.monitor = monitor\n",
    "        self.make_it_fit = make_it_fit\n",
    "        if ckpt_path is not None:\n",
    "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys, only_model=load_only_unet)\n",
    "\n",
    "        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n",
    "                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n",
    "\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        self.learn_logvar = learn_logvar\n",
    "        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n",
    "        if self.learn_logvar:\n",
    "            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n",
    "\n",
    "        self.ucg_training = ucg_training or dict()\n",
    "        if self.ucg_training:\n",
    "            self.ucg_prng = np.random.RandomState()\n",
    "\n",
    "    def register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n",
    "                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
    "        if exists(given_betas):\n",
    "            betas = given_betas\n",
    "        else:\n",
    "            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n",
    "                                       cosine_s=cosine_s)\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "        self.linear_start = linear_start\n",
    "        self.linear_end = linear_end\n",
    "        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n",
    "\n",
    "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
    "\n",
    "        self.register_buffer('betas', to_torch(betas))\n",
    "        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n",
    "        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n",
    "                    1. - alphas_cumprod) + self.v_posterior * betas\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n",
    "        self.register_buffer('posterior_mean_coef1', to_torch(\n",
    "            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n",
    "        self.register_buffer('posterior_mean_coef2', to_torch(\n",
    "            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n",
    "\n",
    "        if self.parameterization == \"eps\":\n",
    "            lvlb_weights = self.betas ** 2 / (\n",
    "                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n",
    "        elif self.parameterization == \"x0\":\n",
    "            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n",
    "        else:\n",
    "            raise NotImplementedError(\"mu not supported\")\n",
    "        # TODO how to choose this term\n",
    "        lvlb_weights[0] = lvlb_weights[1]\n",
    "        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n",
    "        assert not torch.isnan(self.lvlb_weights).all()\n",
    "\n",
    "    @contextmanager\n",
    "    def ema_scope(self, context=None):\n",
    "        if self.use_ema:\n",
    "            self.model_ema.store(self.model.parameters())\n",
    "            self.model_ema.copy_to(self.model)\n",
    "            if context is not None:\n",
    "                print(f\"{context}: Switched to EMA weights\")\n",
    "        try:\n",
    "            yield None\n",
    "        finally:\n",
    "            if self.use_ema:\n",
    "                self.model_ema.restore(self.model.parameters())\n",
    "                if context is not None:\n",
    "                    print(f\"{context}: Restored training weights\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n",
    "        sd = torch.load(path, map_location=\"cpu\")\n",
    "        if \"state_dict\" in list(sd.keys()):\n",
    "            sd = sd[\"state_dict\"]\n",
    "        keys = list(sd.keys())\n",
    "        for k in keys:\n",
    "            for ik in ignore_keys:\n",
    "                if k.startswith(ik):\n",
    "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
    "                    del sd[k]\n",
    "        if self.make_it_fit:\n",
    "            n_params = len([name for name, _ in\n",
    "                            itertools.chain(self.named_parameters(),\n",
    "                                            self.named_buffers())])\n",
    "            for name, param in tqdm(\n",
    "                    itertools.chain(self.named_parameters(),\n",
    "                                    self.named_buffers()),\n",
    "                    desc=\"Fitting old weights to new weights\",\n",
    "                    total=n_params\n",
    "            ):\n",
    "                if not name in sd:\n",
    "                    continue\n",
    "                old_shape = sd[name].shape\n",
    "                new_shape = param.shape\n",
    "                assert len(old_shape)==len(new_shape)\n",
    "                if len(new_shape) > 2:\n",
    "                    # we only modify first two axes\n",
    "                    assert new_shape[2:] == old_shape[2:]\n",
    "                # assumes first axis corresponds to output dim\n",
    "                if not new_shape == old_shape:\n",
    "                    new_param = param.clone()\n",
    "                    old_param = sd[name]\n",
    "                    if len(new_shape) == 1:\n",
    "                        for i in range(new_param.shape[0]):\n",
    "                            new_param[i] = old_param[i % old_shape[0]]\n",
    "                    elif len(new_shape) >= 2:\n",
    "                        for i in range(new_param.shape[0]):\n",
    "                            for j in range(new_param.shape[1]):\n",
    "                                new_param[i, j] = old_param[i % old_shape[0], j % old_shape[1]]\n",
    "\n",
    "                        n_used_old = torch.ones(old_shape[1])\n",
    "                        for j in range(new_param.shape[1]):\n",
    "                            n_used_old[j % old_shape[1]] += 1\n",
    "                        n_used_new = torch.zeros(new_shape[1])\n",
    "                        for j in range(new_param.shape[1]):\n",
    "                            n_used_new[j] = n_used_old[j % old_shape[1]]\n",
    "\n",
    "                        n_used_new = n_used_new[None, :]\n",
    "                        while len(n_used_new.shape) < len(new_shape):\n",
    "                            n_used_new = n_used_new.unsqueeze(-1)\n",
    "                        new_param /= n_used_new\n",
    "\n",
    "                    sd[name] = new_param\n",
    "\n",
    "        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n",
    "            sd, strict=False)\n",
    "        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
    "        if len(missing) > 0:\n",
    "            print(f\"Missing Keys: {missing}\")\n",
    "        if len(unexpected) > 0:\n",
    "            print(f\"Unexpected Keys: {unexpected}\")\n",
    "\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        \"\"\"\n",
    "        Get the distribution q(x_t | x_0).\n",
    "        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n",
    "        \"\"\"\n",
    "        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n",
    "        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def p_mean_variance(self, x, t, clip_denoised: bool):\n",
    "        model_out = self.model(x, t)\n",
    "        if self.parameterization == \"eps\":\n",
    "            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n",
    "        elif self.parameterization == \"x0\":\n",
    "            x_recon = model_out\n",
    "        if clip_denoised:\n",
    "            x_recon.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n",
    "        noise = noise_like(x.shape, device, repeat_noise)\n",
    "        # no noise when t == 0\n",
    "        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
    "        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, return_intermediates=False):\n",
    "        device = self.betas.device\n",
    "        b = shape[0]\n",
    "        img = torch.randn(shape, device=device)\n",
    "        intermediates = [img]\n",
    "        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n",
    "            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n",
    "                                clip_denoised=self.clip_denoised)\n",
    "            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n",
    "                intermediates.append(img)\n",
    "        if return_intermediates:\n",
    "            return img, intermediates\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=16, return_intermediates=False):\n",
    "        image_size = self.image_size\n",
    "        channels = self.channels\n",
    "        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n",
    "                                  return_intermediates=return_intermediates)\n",
    "\n",
    "    def q_sample(self, x_start, t, device, noise=None):\n",
    "        x_start = x_start.to(device)\n",
    "        t = t.to(device)\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start).to(device))  \n",
    "        \n",
    "        sqrt_alphas_cumprod = self.sqrt_alphas_cumprod.to(device)\n",
    "        sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod.to(device)\n",
    "        \n",
    "        return (extract_into_tensor(sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "                extract_into_tensor(sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n",
    "\n",
    "    def get_loss(self, pred, target, mean=True):\n",
    "        if self.loss_type == 'l1':\n",
    "            loss = (target - pred).abs()\n",
    "            if mean:\n",
    "                loss = loss.mean()\n",
    "        elif self.loss_type == 'l2':\n",
    "            if mean:\n",
    "                loss = torch.nn.functional.mse_loss(target, pred)\n",
    "            else:\n",
    "                loss = torch.nn.functional.mse_loss(target, pred, reduction='none')\n",
    "        else:\n",
    "            raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def p_losses(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        model_out = self.model(x_noisy, t)\n",
    "\n",
    "        loss_dict = {}\n",
    "        if self.parameterization == \"eps\":\n",
    "            target = noise\n",
    "        elif self.parameterization == \"x0\":\n",
    "            target = x_start\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Paramterization {self.parameterization} not yet supported\")\n",
    "\n",
    "        loss = self.get_loss(model_out, target, mean=False).mean(dim=[1, 2, 3])\n",
    "\n",
    "        log_prefix = 'train' if self.training else 'val'\n",
    "\n",
    "        loss_dict.update({f'{log_prefix}/loss_simple': loss.mean()})\n",
    "        loss_simple = loss.mean() * self.l_simple_weight\n",
    "\n",
    "        loss_vlb = (self.lvlb_weights[t] * loss).mean()\n",
    "        loss_dict.update({f'{log_prefix}/loss_vlb': loss_vlb})\n",
    "\n",
    "        loss = loss_simple + self.original_elbo_weight * loss_vlb\n",
    "\n",
    "        loss_dict.update({f'{log_prefix}/loss': loss})\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        # b, c, h, w, device, img_size, = *x.shape, x.device, self.image_size\n",
    "        # assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n",
    "        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n",
    "        return self.p_losses(x, t, *args, **kwargs)\n",
    "\n",
    "    def get_input(self, batch, k):\n",
    "        x = batch[k]\n",
    "        if len(x.shape) == 3:\n",
    "            x = x[..., None]\n",
    "        x = rearrange(x, 'b h w c -> b c h w')\n",
    "        x = x.to(memory_format=torch.contiguous_format).float()\n",
    "        return x\n",
    "\n",
    "    def shared_step(self, batch):\n",
    "        x = self.get_input(batch, self.first_stage_key)\n",
    "        loss, loss_dict = self(x)\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        for k in self.ucg_training:\n",
    "            p = self.ucg_training[k][\"p\"]\n",
    "            val = self.ucg_training[k][\"val\"]\n",
    "            if val is None:\n",
    "                val = \"\"\n",
    "            for i in range(len(batch[k])):\n",
    "                if self.ucg_prng.choice(2, p=[1-p, p]):\n",
    "                    batch[k][i] = val\n",
    "\n",
    "        loss, loss_dict = self.shared_step(batch)\n",
    "\n",
    "        self.log_dict(loss_dict, prog_bar=True,\n",
    "                      logger=True, on_step=True, on_epoch=True)\n",
    "\n",
    "        self.log(\"global_step\", self.global_step,\n",
    "                 prog_bar=True, logger=True, on_step=True, on_epoch=False)\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            lr = self.optimizers().param_groups[0]['lr']\n",
    "            self.log('lr_abs', lr, prog_bar=True, logger=True, on_step=True, on_epoch=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, loss_dict_no_ema = self.shared_step(batch)\n",
    "        with self.ema_scope():\n",
    "            _, loss_dict_ema = self.shared_step(batch)\n",
    "            loss_dict_ema = {key + '_ema': loss_dict_ema[key] for key in loss_dict_ema}\n",
    "        self.log_dict(loss_dict_no_ema, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n",
    "        self.log_dict(loss_dict_ema, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def on_train_batch_end(self, *args, **kwargs):\n",
    "        if self.use_ema:\n",
    "            self.model_ema(self.model)\n",
    "\n",
    "    def _get_rows_from_list(self, samples):\n",
    "        n_imgs_per_row = len(samples)\n",
    "        denoise_grid = rearrange(samples, 'n b c h w -> b n c h w')\n",
    "        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n",
    "        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n",
    "        return denoise_grid\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def log_images(self, batch, N=8, n_row=2, sample=True, return_keys=None, **kwargs):\n",
    "        log = dict()\n",
    "        x = self.get_input(batch, self.first_stage_key)\n",
    "        N = min(x.shape[0], N)\n",
    "        n_row = min(x.shape[0], n_row)\n",
    "        x = x.to(self.device)[:N]\n",
    "        log[\"inputs\"] = x\n",
    "\n",
    "        # get diffusion row\n",
    "        diffusion_row = list()\n",
    "        x_start = x[:n_row]\n",
    "\n",
    "        for t in range(self.num_timesteps):\n",
    "            if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n",
    "                t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n",
    "                t = t.to(self.device).long()\n",
    "                noise = torch.randn_like(x_start)\n",
    "                x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "                diffusion_row.append(x_noisy)\n",
    "\n",
    "        log[\"diffusion_row\"] = self._get_rows_from_list(diffusion_row)\n",
    "\n",
    "        if sample:\n",
    "            # get denoise row\n",
    "            with self.ema_scope(\"Plotting\"):\n",
    "                samples, denoise_row = self.sample(batch_size=N, return_intermediates=True)\n",
    "\n",
    "            log[\"samples\"] = samples\n",
    "            log[\"denoise_row\"] = self._get_rows_from_list(denoise_row)\n",
    "\n",
    "        if return_keys:\n",
    "            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:\n",
    "                return log\n",
    "            else:\n",
    "                return {key: log[key] for key in return_keys}\n",
    "        return log\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = 1.0e-06\n",
    "        params = list(self.model.parameters())\n",
    "        if self.learn_logvar:\n",
    "            params = params + [self.logvar]\n",
    "        opt = torch.optim.AdamW(params, lr=lr)\n",
    "        return opt\n",
    "\n",
    "class LatentDiffusion(DDPM):\n",
    "    \"\"\"main class\"\"\"\n",
    "    def __init__(self,\n",
    "                 first_stage_config,\n",
    "                 cond_stage_config,\n",
    "                 num_timesteps_cond=None,\n",
    "                 cond_stage_key=\"image\",\n",
    "                 cond_stage_trainable=False,\n",
    "                 concat_mode=True,\n",
    "                 cond_stage_forward=None,\n",
    "                 conditioning_key=None,\n",
    "                 scale_factor=1.0,\n",
    "                 scale_by_std=False,\n",
    "                 *args, **kwargs):\n",
    "        self.num_timesteps_cond = default(num_timesteps_cond, 1)\n",
    "        self.scale_by_std = scale_by_std\n",
    "        assert self.num_timesteps_cond <= kwargs['timesteps']\n",
    "        # for backwards compatibility after implementation of DiffusionWrapper\n",
    "        if conditioning_key is None:\n",
    "            conditioning_key = 'concat' if concat_mode else 'crossattn'\n",
    "        if cond_stage_config == '__is_unconditional__':\n",
    "            conditioning_key = None\n",
    "        ckpt_path = kwargs.pop(\"ckpt_path\", None)\n",
    "        ignore_keys = kwargs.pop(\"ignore_keys\", [])\n",
    "        super().__init__(conditioning_key=conditioning_key, *args, **kwargs)\n",
    "        self.concat_mode = concat_mode\n",
    "        self.cond_stage_trainable = cond_stage_trainable\n",
    "        self.cond_stage_key = cond_stage_key\n",
    "        try:\n",
    "            self.num_downs = len(first_stage_config.params.ddconfig.ch_mult) - 1\n",
    "        except:\n",
    "            self.num_downs = 0\n",
    "        if not scale_by_std:\n",
    "            self.scale_factor = scale_factor\n",
    "        else:\n",
    "            self.register_buffer('scale_factor', torch.tensor(scale_factor))\n",
    "        self.instantiate_first_stage(first_stage_config)\n",
    "        self.instantiate_cond_stage(cond_stage_config)\n",
    "        self.cond_stage_forward = cond_stage_forward\n",
    "        self.clip_denoised = False\n",
    "        self.bbox_tokenizer = None  \n",
    "\n",
    "        self.restarted_from_ckpt = False\n",
    "        if ckpt_path is not None:\n",
    "            self.init_from_ckpt(ckpt_path, ignore_keys)\n",
    "            self.restarted_from_ckpt = True\n",
    "\n",
    "    def make_cond_schedule(self, ):\n",
    "        self.cond_ids = torch.full(size=(self.num_timesteps,), fill_value=self.num_timesteps - 1, dtype=torch.long)\n",
    "        ids = torch.round(torch.linspace(0, self.num_timesteps - 1, self.num_timesteps_cond)).long()\n",
    "        self.cond_ids[:self.num_timesteps_cond] = ids\n",
    "\n",
    "    @rank_zero_only\n",
    "    @torch.no_grad()\n",
    "    def on_train_batch_start(self, batch, batch_idx, dataloader_idx):\n",
    "        # only for very first batch\n",
    "        if self.scale_by_std and self.current_epoch == 0 and self.global_step == 0 and batch_idx == 0 and not self.restarted_from_ckpt:\n",
    "            assert self.scale_factor == 1., 'rather not use custom rescaling and std-rescaling simultaneously'\n",
    "            # set rescale weight to 1./std of encodings\n",
    "            print(\"### USING STD-RESCALING ###\")\n",
    "            x = super().get_input(batch, self.first_stage_key)\n",
    "            x = x.to(self.device)\n",
    "            encoder_posterior = self.encode_first_stage(x)\n",
    "            z = self.get_first_stage_encoding(encoder_posterior).detach()\n",
    "            del self.scale_factor\n",
    "            self.register_buffer('scale_factor', 1. / z.flatten().std())\n",
    "            print(f\"setting self.scale_factor to {self.scale_factor}\")\n",
    "            print(\"### USING STD-RESCALING ###\")\n",
    "\n",
    "    def register_schedule(self,\n",
    "                          given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n",
    "                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
    "        super().register_schedule(given_betas, beta_schedule, timesteps, linear_start, linear_end, cosine_s)\n",
    "\n",
    "        self.shorten_cond_schedule = self.num_timesteps_cond > 1\n",
    "        if self.shorten_cond_schedule:\n",
    "            self.make_cond_schedule()\n",
    "\n",
    "    def instantiate_first_stage(self, config):\n",
    "        device = config.get('device', 'cpu')\n",
    "        model = instantiate_from_config(config)\n",
    "        self.first_stage_model = model.eval()\n",
    "        self.first_stage_model.to(device)\n",
    "        self.first_stage_model.train = disabled_train\n",
    "        for param in self.first_stage_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def instantiate_cond_stage(self, config):\n",
    "        device = config.get('device', 'cpu')\n",
    "        if not self.cond_stage_trainable:\n",
    "            if config == \"__is_first_stage__\":\n",
    "                print(\"Using first stage also as cond stage.\")\n",
    "                self.cond_stage_model = self.first_stage_model\n",
    "            elif config == \"__is_unconditional__\":\n",
    "                print(f\"Training {self.__class__.__name__} as an unconditional model.\")\n",
    "                self.cond_stage_model = None\n",
    "                # self.be_unconditional = True\n",
    "            else:\n",
    "                model = instantiate_from_config(config)\n",
    "                self.cond_stage_model = model.eval()\n",
    "                self.cond_stage_model.train = disabled_train\n",
    "                for param in self.cond_stage_model.parameters():\n",
    "                    param.requires_grad = False\n",
    "        else:\n",
    "            assert config != '__is_first_stage__'\n",
    "            assert config != '__is_unconditional__'\n",
    "            model = instantiate_from_config(config)\n",
    "            self.cond_stage_model = model.to(device)\n",
    "\n",
    "    def _get_denoise_row_from_list(self, samples, desc='', force_no_decoder_quantization=False):\n",
    "        denoise_row = []\n",
    "        for zd in tqdm(samples, desc=desc):\n",
    "            denoise_row.append(self.decode_first_stage(zd.to(self.device),\n",
    "                                                            force_not_quantize=force_no_decoder_quantization))\n",
    "        n_imgs_per_row = len(denoise_row)\n",
    "        denoise_row = torch.stack(denoise_row)  # n_log_step, n_row, C, H, W\n",
    "        denoise_grid = rearrange(denoise_row, 'n b c h w -> b n c h w')\n",
    "        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n",
    "        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n",
    "        return denoise_grid\n",
    "\n",
    "    def get_first_stage_encoding(self, encoder_posterior):\n",
    "        if isinstance(encoder_posterior, DiagonalGaussianDistribution):\n",
    "            z = encoder_posterior.sample()\n",
    "        elif isinstance(encoder_posterior, torch.Tensor):\n",
    "            z = encoder_posterior\n",
    "        else:\n",
    "            raise NotImplementedError(f\"encoder_posterior of type '{type(encoder_posterior)}' not yet implemented\")\n",
    "        return self.scale_factor * z\n",
    "\n",
    "    def get_learned_conditioning(self, c):\n",
    "        if self.cond_stage_forward is None:\n",
    "            if hasattr(self.cond_stage_model, 'encode') and callable(self.cond_stage_model.encode):\n",
    "                c = self.cond_stage_model.encode(c)\n",
    "                if isinstance(c, DiagonalGaussianDistribution):\n",
    "                    c = c.mode()\n",
    "            else:\n",
    "                c = self.cond_stage_model(c)\n",
    "        else:\n",
    "            assert hasattr(self.cond_stage_model, self.cond_stage_forward)\n",
    "            c = getattr(self.cond_stage_model, self.cond_stage_forward)(c)\n",
    "        return c\n",
    "\n",
    "    def meshgrid(self, h, w):\n",
    "        y = torch.arange(0, h).view(h, 1, 1).repeat(1, w, 1)\n",
    "        x = torch.arange(0, w).view(1, w, 1).repeat(h, 1, 1)\n",
    "\n",
    "        arr = torch.cat([y, x], dim=-1)\n",
    "        return arr\n",
    "\n",
    "    def delta_border(self, h, w):\n",
    "        \"\"\"\n",
    "        :param h: height\n",
    "        :param w: width\n",
    "        :return: normalized distance to image border,\n",
    "         wtith min distance = 0 at border and max dist = 0.5 at image center\n",
    "        \"\"\"\n",
    "        lower_right_corner = torch.tensor([h - 1, w - 1]).view(1, 1, 2)\n",
    "        arr = self.meshgrid(h, w) / lower_right_corner\n",
    "        dist_left_up = torch.min(arr, dim=-1, keepdims=True)[0]\n",
    "        dist_right_down = torch.min(1 - arr, dim=-1, keepdims=True)[0]\n",
    "        edge_dist = torch.min(torch.cat([dist_left_up, dist_right_down], dim=-1), dim=-1)[0]\n",
    "        return edge_dist\n",
    "\n",
    "    def get_weighting(self, h, w, Ly, Lx, device):\n",
    "        weighting = self.delta_border(h, w)\n",
    "        weighting = torch.clip(weighting, self.split_input_params[\"clip_min_weight\"],\n",
    "                               self.split_input_params[\"clip_max_weight\"], )\n",
    "        weighting = weighting.view(1, h * w, 1).repeat(1, 1, Ly * Lx).to(device)\n",
    "\n",
    "        if self.split_input_params[\"tie_braker\"]:\n",
    "            L_weighting = self.delta_border(Ly, Lx)\n",
    "            L_weighting = torch.clip(L_weighting,\n",
    "                                     self.split_input_params[\"clip_min_tie_weight\"],\n",
    "                                     self.split_input_params[\"clip_max_tie_weight\"])\n",
    "\n",
    "            L_weighting = L_weighting.view(1, 1, Ly * Lx).to(device)\n",
    "            weighting = weighting * L_weighting\n",
    "        return weighting\n",
    "\n",
    "    def get_fold_unfold(self, x, kernel_size, stride, uf=1, df=1):  # todo load once not every time, shorten code\n",
    "        \"\"\"\n",
    "        :param x: img of size (bs, c, h, w)\n",
    "        :return: n img crops of size (n, bs, c, kernel_size[0], kernel_size[1])\n",
    "        \"\"\"\n",
    "        bs, nc, h, w = x.shape\n",
    "\n",
    "        # number of crops in image\n",
    "        Ly = (h - kernel_size[0]) // stride[0] + 1\n",
    "        Lx = (w - kernel_size[1]) // stride[1] + 1\n",
    "\n",
    "        if uf == 1 and df == 1:\n",
    "            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
    "            unfold = torch.nn.Unfold(**fold_params)\n",
    "\n",
    "            fold = torch.nn.Fold(output_size=x.shape[2:], **fold_params)\n",
    "\n",
    "            weighting = self.get_weighting(kernel_size[0], kernel_size[1], Ly, Lx, x.device).to(x.dtype)\n",
    "            normalization = fold(weighting).view(1, 1, h, w)  # normalizes the overlap\n",
    "            weighting = weighting.view((1, 1, kernel_size[0], kernel_size[1], Ly * Lx))\n",
    "\n",
    "        elif uf > 1 and df == 1:\n",
    "            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
    "            unfold = torch.nn.Unfold(**fold_params)\n",
    "\n",
    "            fold_params2 = dict(kernel_size=(kernel_size[0] * uf, kernel_size[0] * uf),\n",
    "                                dilation=1, padding=0,\n",
    "                                stride=(stride[0] * uf, stride[1] * uf))\n",
    "            fold = torch.nn.Fold(output_size=(x.shape[2] * uf, x.shape[3] * uf), **fold_params2)\n",
    "\n",
    "            weighting = self.get_weighting(kernel_size[0] * uf, kernel_size[1] * uf, Ly, Lx, x.device).to(x.dtype)\n",
    "            normalization = fold(weighting).view(1, 1, h * uf, w * uf)  # normalizes the overlap\n",
    "            weighting = weighting.view((1, 1, kernel_size[0] * uf, kernel_size[1] * uf, Ly * Lx))\n",
    "\n",
    "        elif df > 1 and uf == 1:\n",
    "            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
    "            unfold = torch.nn.Unfold(**fold_params)\n",
    "\n",
    "            fold_params2 = dict(kernel_size=(kernel_size[0] // df, kernel_size[0] // df),\n",
    "                                dilation=1, padding=0,\n",
    "                                stride=(stride[0] // df, stride[1] // df))\n",
    "            fold = torch.nn.Fold(output_size=(x.shape[2] // df, x.shape[3] // df), **fold_params2)\n",
    "\n",
    "            weighting = self.get_weighting(kernel_size[0] // df, kernel_size[1] // df, Ly, Lx, x.device).to(x.dtype)\n",
    "            normalization = fold(weighting).view(1, 1, h // df, w // df)  # normalizes the overlap\n",
    "            weighting = weighting.view((1, 1, kernel_size[0] // df, kernel_size[1] // df, Ly * Lx))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return fold, unfold, normalization, weighting\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_input(self, batch, k, device, return_first_stage_outputs=False, force_c_encode=False,\n",
    "                  cond_key=None, return_original_cond=False, bs=None, return_x=False):\n",
    "        \n",
    "        x = super().get_input(batch, k)\n",
    "        # print(\"ldmm\",x.shape)\n",
    "        if bs is not None:\n",
    "            x = x[:bs]\n",
    "        x = x.to(device)\n",
    "\n",
    "        ### Encoder\n",
    "        encoder_posterior = self.encode_first_stage(x)\n",
    "        z = self.get_first_stage_encoding(encoder_posterior).detach()\n",
    "        \n",
    "        if self.model.conditioning_key is not None:\n",
    "            if cond_key is None:\n",
    "                cond_key = self.cond_stage_key\n",
    "            if cond_key != self.first_stage_key:\n",
    "                if cond_key in ['caption', 'coordinates_bbox', \"txt\"]:\n",
    "                    xc = batch[cond_key]\n",
    "                elif cond_key == 'class_label':\n",
    "                    xc = batch\n",
    "                else:\n",
    "                    xc = super().get_input(batch, cond_key).to(device)\n",
    "            else:\n",
    "                xc = x\n",
    "\n",
    "            if not self.cond_stage_trainable or force_c_encode:\n",
    "                if isinstance(xc, dict) or isinstance(xc, list):\n",
    "                    c = self.get_learned_conditioning(xc)\n",
    "                else:\n",
    "                    c = self.get_learned_conditioning(xc.to(device))\n",
    "            else:\n",
    "                c = xc\n",
    "            if bs is not None:\n",
    "                c = c[:bs]\n",
    "\n",
    "            if self.use_positional_encodings:\n",
    "                pos_x, pos_y = self.compute_latent_shifts(batch)\n",
    "                ckey = __conditioning_keys__[self.model.conditioning_key]\n",
    "                c = {ckey: c, 'pos_x': pos_x, 'pos_y': pos_y}\n",
    "            if isinstance(c, dict):\n",
    "                for key in c:\n",
    "                    if key == \"class_label\":\n",
    "                        c[key] = c[key].to(device)\n",
    "        else:\n",
    "            c = None\n",
    "            xc = None\n",
    "            if self.use_positional_encodings:\n",
    "                pos_x, pos_y = self.compute_latent_shifts(batch)\n",
    "                c = {'pos_x': pos_x, 'pos_y': pos_y}\n",
    "        out = [z, c]\n",
    "        if return_first_stage_outputs:\n",
    "            xrec = self.decode_first_stage(z)\n",
    "            out.extend([x, xrec])\n",
    "        if return_x:\n",
    "            out.extend([x])\n",
    "        if return_original_cond:\n",
    "            out.append(xc)\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n",
    "        if predict_cids:\n",
    "            if z.dim() == 4:\n",
    "                z = torch.argmax(z.exp(), dim=1).long()\n",
    "            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n",
    "            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n",
    "\n",
    "        z = 1. / self.scale_factor * z\n",
    "\n",
    "        if hasattr(self, \"split_input_params\"):\n",
    "            if self.split_input_params[\"patch_distributed_vq\"]:\n",
    "                ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n",
    "                stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n",
    "                uf = self.split_input_params[\"vqf\"]\n",
    "                bs, nc, h, w = z.shape\n",
    "                if ks[0] > h or ks[1] > w:\n",
    "                    ks = (min(ks[0], h), min(ks[1], w))\n",
    "                    print(\"reducing Kernel\")\n",
    "\n",
    "                if stride[0] > h or stride[1] > w:\n",
    "                    stride = (min(stride[0], h), min(stride[1], w))\n",
    "                    print(\"reducing stride\")\n",
    "\n",
    "                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n",
    "\n",
    "                z = unfold(z)  # (bn, nc * prod(**ks), L)\n",
    "                # 1. Reshape to img shape\n",
    "                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
    "\n",
    "                # 2. apply model loop over last dim\n",
    "                if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n",
    "                                                                 force_not_quantize=predict_cids or force_not_quantize)\n",
    "                                   for i in range(z.shape[-1])]\n",
    "                else:\n",
    "\n",
    "                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n",
    "                                   for i in range(z.shape[-1])]\n",
    "\n",
    "                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n",
    "                o = o * weighting\n",
    "                # Reverse 1. reshape to img shape\n",
    "                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
    "                # stitch crops together\n",
    "                decoded = fold(o)\n",
    "                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n",
    "                return decoded\n",
    "            else:\n",
    "                if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n",
    "                else:\n",
    "                    return self.first_stage_model.decode(z)\n",
    "\n",
    "        else:\n",
    "            if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n",
    "            else:\n",
    "                return self.first_stage_model.decode(z)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_first_stage(self, x):\n",
    "        if hasattr(self, \"split_input_params\"):\n",
    "            if self.split_input_params[\"patch_distributed_vq\"]:\n",
    "                ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n",
    "                stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n",
    "                df = self.split_input_params[\"vqf\"]\n",
    "                self.split_input_params['original_image_size'] = x.shape[-2:]\n",
    "                bs, nc, h, w = x.shape\n",
    "                if ks[0] > h or ks[1] > w:\n",
    "                    ks = (min(ks[0], h), min(ks[1], w))\n",
    "                    print(\"reducing Kernel\")\n",
    "\n",
    "                if stride[0] > h or stride[1] > w:\n",
    "                    stride = (min(stride[0], h), min(stride[1], w))\n",
    "                    print(\"reducing stride\")\n",
    "\n",
    "                fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df)\n",
    "                z = unfold(x)  # (bn, nc * prod(**ks), L)\n",
    "                # Reshape to img shape\n",
    "                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
    "\n",
    "                output_list = [self.first_stage_model.encode(z[:, :, :, :, i])\n",
    "                               for i in range(z.shape[-1])]\n",
    "\n",
    "                o = torch.stack(output_list, axis=-1)\n",
    "                o = o * weighting\n",
    "\n",
    "                # Reverse reshape to img shape\n",
    "                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
    "                # stitch crops together\n",
    "                decoded = fold(o)\n",
    "                decoded = decoded / normalization\n",
    "                return decoded\n",
    "\n",
    "            else:\n",
    "                return self.first_stage_model.encode(x)\n",
    "        else:\n",
    "            return self.first_stage_model.encode(x)\n",
    "\n",
    "    def shared_step(self, batch, **kwargs):\n",
    "        x,c = self.get_input(batch,self.first_stage_key)\n",
    "        loss = self(x, c)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x, c, *args, **kwargs):\n",
    "        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n",
    "        if self.model.conditioning_key is not None:\n",
    "            assert c is not None\n",
    "            if self.cond_stage_trainable:\n",
    "                c = self.get_learned_conditioning(c)\n",
    "            if self.shorten_cond_schedule:  # TODO: drop this option\n",
    "                tc = self.cond_ids[t].to(self.device)\n",
    "                c = self.q_sample(x_start=c, t=tc, noise=torch.randn_like(c.float()))\n",
    "        return self.p_losses(x, c, t, *args, **kwargs)\n",
    "\n",
    "    def _rescale_annotations(self, bboxes, crop_coordinates):  # TODO: move to dataset\n",
    "        def rescale_bbox(bbox):\n",
    "            x0 = clamp((bbox[0] - crop_coordinates[0]) / crop_coordinates[2])\n",
    "            y0 = clamp((bbox[1] - crop_coordinates[1]) / crop_coordinates[3])\n",
    "            w = min(bbox[2] / crop_coordinates[2], 1 - x0)\n",
    "            h = min(bbox[3] / crop_coordinates[3], 1 - y0)\n",
    "            return x0, y0, w, h\n",
    "\n",
    "        return [rescale_bbox(b) for b in bboxes]\n",
    "\n",
    "    def apply_model(self, x_noisy, t, cond, device, return_ids=False):\n",
    "\n",
    "        if isinstance(cond, dict):\n",
    "            # hybrid case, cond is exptected to be a dict\n",
    "            pass\n",
    "        else:\n",
    "            if not isinstance(cond, list):\n",
    "                cond = [cond]\n",
    "            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n",
    "            cond = {key: cond}\n",
    "        if hasattr(self, \"split_input_params\"):\n",
    "            assert len(cond) == 1  # todo can only deal with one conditioning atm\n",
    "            assert not return_ids  \n",
    "            ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n",
    "            stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n",
    "\n",
    "            h, w = x_noisy.shape[-2:]\n",
    "\n",
    "            fold, unfold, normalization, weighting = self.get_fold_unfold(x_noisy, ks, stride)\n",
    "\n",
    "            z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n",
    "            # Reshape to img shape\n",
    "            z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
    "            z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n",
    "\n",
    "            if self.cond_stage_key in [\"image\", \"LR_image\", \"segmentation\",\n",
    "                                       'bbox_img'] and self.model.conditioning_key:  # todo check for completeness\n",
    "                c_key = next(iter(cond.keys()))  # get key\n",
    "                c = next(iter(cond.values()))  # get value\n",
    "                assert (len(c) == 1)  # todo extend to list with more than one elem\n",
    "                c = c[0]  # get element\n",
    "\n",
    "                c = unfold(c)\n",
    "                c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
    "\n",
    "                cond_list = [{c_key: [c[:, :, :, :, i]]} for i in range(c.shape[-1])]\n",
    "\n",
    "            elif self.cond_stage_key == 'coordinates_bbox':\n",
    "                assert 'original_image_size' in self.split_input_params, 'BoudingBoxRescaling is missing original_image_size'\n",
    "\n",
    "                # assuming padding of unfold is always 0 and its dilation is always 1\n",
    "                n_patches_per_row = int((w - ks[0]) / stride[0] + 1)\n",
    "                full_img_h, full_img_w = self.split_input_params['original_image_size']\n",
    "                # as we are operating on latents, we need the factor from the original image size to the\n",
    "                # spatial latent size to properly rescale the crops for regenerating the bbox annotations\n",
    "                num_downs = self.first_stage_model.encoder.num_resolutions - 1\n",
    "                rescale_latent = 2 ** (num_downs)\n",
    "\n",
    "                # get top left postions of patches as conforming for the bbbox tokenizer, therefore we\n",
    "                # need to rescale the tl patch coordinates to be in between (0,1)\n",
    "                tl_patch_coordinates = [(rescale_latent * stride[0] * (patch_nr % n_patches_per_row) / full_img_w,\n",
    "                                         rescale_latent * stride[1] * (patch_nr // n_patches_per_row) / full_img_h)\n",
    "                                        for patch_nr in range(z.shape[-1])]\n",
    "\n",
    "                # patch_limits are tl_coord, width and height coordinates as (x_tl, y_tl, h, w)\n",
    "                patch_limits = [(x_tl, y_tl,\n",
    "                                 rescale_latent * ks[0] / full_img_w,\n",
    "                                 rescale_latent * ks[1] / full_img_h) for x_tl, y_tl in tl_patch_coordinates]\n",
    "                # patch_values = [(np.arange(x_tl,min(x_tl+ks, 1.)),np.arange(y_tl,min(y_tl+ks, 1.))) for x_tl, y_tl in tl_patch_coordinates]\n",
    "\n",
    "                # tokenize crop coordinates for the bounding boxes of the respective patches\n",
    "                patch_limits_tknzd = [torch.LongTensor(self.bbox_tokenizer._crop_encoder(bbox))[None].to(self.device)\n",
    "                                      for bbox in patch_limits]  # list of length l with tensors of shape (1, 2)\n",
    "                print(patch_limits_tknzd[0].shape)\n",
    "                # cut tknzd crop position from conditioning\n",
    "                assert isinstance(cond, dict), 'cond must be dict to be fed into model'\n",
    "                cut_cond = cond['c_crossattn'][0][..., :-2].to(self.device)\n",
    "                print(cut_cond.shape)\n",
    "\n",
    "                adapted_cond = torch.stack([torch.cat([cut_cond, p], dim=1) for p in patch_limits_tknzd])\n",
    "                adapted_cond = rearrange(adapted_cond, 'l b n -> (l b) n')\n",
    "                print(adapted_cond.shape)\n",
    "                adapted_cond = self.get_learned_conditioning(adapted_cond)\n",
    "                print(adapted_cond.shape)\n",
    "                adapted_cond = rearrange(adapted_cond, '(l b) n d -> l b n d', l=z.shape[-1])\n",
    "                print(adapted_cond.shape)\n",
    "\n",
    "                cond_list = [{'c_crossattn': [e]} for e in adapted_cond]\n",
    "\n",
    "            else:\n",
    "                cond_list = [cond for i in range(z.shape[-1])]  # Todo make this more efficient\n",
    "\n",
    "            # apply model by loop over crops\n",
    "            output_list = [self.model(z_list[i], t, **cond_list[i]) for i in range(z.shape[-1])]\n",
    "            assert not isinstance(output_list[0],\n",
    "                                  tuple)  # todo cant deal with multiple model outputs check this never happens\n",
    "\n",
    "            o = torch.stack(output_list, axis=-1)\n",
    "            o = o * weighting\n",
    "            # Reverse reshape to img shape\n",
    "            o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
    "            # stitch crops together\n",
    "            x_recon = fold(o) / normalization\n",
    "\n",
    "        else:\n",
    "            x_noisy = x_noisy.to(device)\n",
    "            t = t.to(device)\n",
    "            \n",
    "            if isinstance(cond, dict):\n",
    "                for key in cond:\n",
    "                    if isinstance(cond[key], list):\n",
    "                        cond[key] = [item.to(device) for item in cond[key]]\n",
    "                    else:\n",
    "                        cond[key] = cond[key].to(device)\n",
    "            else:\n",
    "                cond = cond.to(device)\n",
    "            \n",
    "            self.model.to(device)\n",
    "            x_recon = self.model(x_noisy, t, **cond)\n",
    "\n",
    "        if isinstance(x_recon, tuple) and not return_ids:\n",
    "            return x_recon[0]\n",
    "        else:\n",
    "            return x_recon\n",
    "\n",
    "    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n",
    "        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n",
    "               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "\n",
    "    def _prior_bpd(self, x_start):\n",
    "        \"\"\"\n",
    "        Get the prior KL term for the variational lower-bound, measured in\n",
    "        bits-per-dim.\n",
    "        This term can't be optimized, as it only depends on the encoder.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs.\n",
    "        :return: a batch of [N] KL values (in bits), one per batch element.\n",
    "        \"\"\"\n",
    "        batch_size = x_start.shape[0]\n",
    "        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n",
    "        qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n",
    "        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n",
    "        return mean_flat(kl_prior) / np.log(2.0)\n",
    "\n",
    "    def p_losses(self, x_start, cond, t, device, noise=None):\n",
    "        # noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        # ###  Unet\n",
    "        # x_noisy = self.q_sample(x_start=x_start, t=t,device=device, noise=noise)\n",
    "        # model_output = self.apply_model(x_noisy, t, cond, device)\n",
    "\n",
    "        # loss_dict = {}\n",
    "        # prefix = 'train' if self.training else 'val'\n",
    "\n",
    "        # if self.parameterization == \"x0\":\n",
    "        #     target = x_start\n",
    "        # elif self.parameterization == \"eps\":\n",
    "        #     target = noise\n",
    "        # else:\n",
    "        #     raise NotImplementedError()\n",
    "\n",
    "        # loss_simple = self.get_loss(model_output, target, mean=False).mean([1, 2, 3])\n",
    "        # loss_dict.update({f'{prefix}/loss_simple': loss_simple.mean()})\n",
    "\n",
    "        # logvar_t = self.logvar[t].to(device)\n",
    "        # loss = loss_simple / torch.exp(logvar_t) + logvar_t\n",
    "        # # loss = loss_simple / torch.exp(self.logvar) + self.logvar\n",
    "        # if self.learn_logvar:\n",
    "        #     loss_dict.update({f'{prefix}/loss_gamma': loss.mean()})\n",
    "        #     loss_dict.update({'logvar': self.logvar.data.mean()})\n",
    "\n",
    "        # loss = self.l_simple_weight * loss.mean()\n",
    "\n",
    "        # loss_vlb = self.get_loss(model_output, target, mean=False).mean(dim=(1, 2, 3))\n",
    "        # loss_vlb = (self.lvlb_weights[t] * loss_vlb).mean()\n",
    "        # loss_dict.update({f'{prefix}/loss_vlb': loss_vlb})\n",
    "        # loss += (self.original_elbo_weight * loss_vlb)\n",
    "        # loss_dict.update({f'{prefix}/loss': loss})\n",
    "\n",
    "        # return loss, loss_dict\n",
    "        device = t.device  # t가 위치한 디바이스를 가져옴\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start).to(device))  # noise를 t와 동일한 디바이스로 이동\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, device=device, noise=noise)\n",
    "        model_output = self.apply_model(x_noisy, t, cond, device)\n",
    "\n",
    "        loss_dict = {}\n",
    "        prefix = 'train' if self.training else 'val'\n",
    "\n",
    "        if self.parameterization == \"x0\":\n",
    "            target = x_start\n",
    "        elif self.parameterization == \"eps\":\n",
    "            target = noise\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        loss_simple = self.get_loss(model_output, target, mean=False).mean([1, 2, 3])\n",
    "        loss_dict.update({f'{prefix}/loss_simple': loss_simple.mean()})\n",
    "\n",
    "        logvar_t = self.logvar[t].to(device)\n",
    "        loss = loss_simple / torch.exp(logvar_t) + logvar_t\n",
    "        if self.learn_logvar:\n",
    "            loss_dict.update({f'{prefix}/loss_gamma': loss.mean()})\n",
    "            loss_dict.update({'logvar': self.logvar.data.mean()})\n",
    "\n",
    "        loss = self.l_simple_weight * loss.mean()\n",
    "\n",
    "        loss_vlb = self.get_loss(model_output, target, mean=False).mean(dim=(1, 2, 3))\n",
    "        # self.lvlb_weights를 t와 동일한 디바이스로 이동\n",
    "        lvlb_weights = self.lvlb_weights.to(device)\n",
    "        loss_vlb = (lvlb_weights[t] * loss_vlb).mean()\n",
    "        loss_dict.update({f'{prefix}/loss_vlb': loss_vlb})\n",
    "        loss += (self.original_elbo_weight * loss_vlb)\n",
    "        loss_dict.update({f'{prefix}/loss': loss})\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def p_mean_variance(self, x, c, t, clip_denoised: bool, return_codebook_ids=False, quantize_denoised=False,\n",
    "                        return_x0=False, score_corrector=None, corrector_kwargs=None):\n",
    "        t_in = t\n",
    "        model_out = self.apply_model(x, t_in, c, return_ids=return_codebook_ids)\n",
    "\n",
    "        if score_corrector is not None:\n",
    "            assert self.parameterization == \"eps\"\n",
    "            model_out = score_corrector.modify_score(self, model_out, x, t, c, **corrector_kwargs)\n",
    "\n",
    "        if return_codebook_ids:\n",
    "            model_out, logits = model_out\n",
    "\n",
    "        if self.parameterization == \"eps\":\n",
    "            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n",
    "        elif self.parameterization == \"x0\":\n",
    "            x_recon = model_out\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_recon.clamp_(-1., 1.)\n",
    "        if quantize_denoised:\n",
    "            x_recon, _, [_, _, indices] = self.first_stage_model.quantize(x_recon)\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n",
    "        if return_codebook_ids:\n",
    "            return model_mean, posterior_variance, posterior_log_variance, logits\n",
    "        elif return_x0:\n",
    "            return model_mean, posterior_variance, posterior_log_variance, x_recon\n",
    "        else:\n",
    "            return model_mean, posterior_variance, posterior_log_variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, c, t, clip_denoised=False, repeat_noise=False,\n",
    "                 return_codebook_ids=False, quantize_denoised=False, return_x0=False,\n",
    "                 temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        outputs = self.p_mean_variance(x=x, c=c, t=t, clip_denoised=clip_denoised,\n",
    "                                       return_codebook_ids=return_codebook_ids,\n",
    "                                       quantize_denoised=quantize_denoised,\n",
    "                                       return_x0=return_x0,\n",
    "                                       score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n",
    "        if return_codebook_ids:\n",
    "            raise DeprecationWarning(\"Support dropped.\")\n",
    "            model_mean, _, model_log_variance, logits = outputs\n",
    "        elif return_x0:\n",
    "            model_mean, _, model_log_variance, x0 = outputs\n",
    "        else:\n",
    "            model_mean, _, model_log_variance = outputs\n",
    "\n",
    "        noise = noise_like(x.shape, device, repeat_noise) * temperature\n",
    "        if noise_dropout > 0.:\n",
    "            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n",
    "        # no noise when t == 0\n",
    "        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
    "\n",
    "        if return_codebook_ids:\n",
    "            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise, logits.argmax(dim=1)\n",
    "        if return_x0:\n",
    "            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise, x0\n",
    "        else:\n",
    "            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def progressive_denoising(self, cond, shape, verbose=True, callback=None, quantize_denoised=False,\n",
    "                              img_callback=None, mask=None, x0=None, temperature=1., noise_dropout=0.,\n",
    "                              score_corrector=None, corrector_kwargs=None, batch_size=None, x_T=None, start_T=None,\n",
    "                              log_every_t=None):\n",
    "        if not log_every_t:\n",
    "            log_every_t = self.log_every_t\n",
    "        timesteps = self.num_timesteps\n",
    "        if batch_size is not None:\n",
    "            b = batch_size if batch_size is not None else shape[0]\n",
    "            shape = [batch_size] + list(shape)\n",
    "        else:\n",
    "            b = batch_size = shape[0]\n",
    "        if x_T is None:\n",
    "            img = torch.randn(shape, device=self.device)\n",
    "        else:\n",
    "            img = x_T\n",
    "        intermediates = []\n",
    "        if cond is not None:\n",
    "            if isinstance(cond, dict):\n",
    "                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n",
    "                list(map(lambda x: x[:batch_size], cond[key])) for key in cond}\n",
    "            else:\n",
    "                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n",
    "\n",
    "        if start_T is not None:\n",
    "            timesteps = min(timesteps, start_T)\n",
    "        iterator = tqdm(reversed(range(0, timesteps)), desc='Progressive Generation',\n",
    "                        total=timesteps) if verbose else reversed(\n",
    "            range(0, timesteps))\n",
    "        if type(temperature) == float:\n",
    "            temperature = [temperature] * timesteps\n",
    "\n",
    "        for i in iterator:\n",
    "            ts = torch.full((b,), i, device=self.device, dtype=torch.long)\n",
    "            if self.shorten_cond_schedule:\n",
    "                assert self.model.conditioning_key != 'hybrid'\n",
    "                tc = self.cond_ids[ts].to(cond.device)\n",
    "                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n",
    "\n",
    "            img, x0_partial = self.p_sample(img, cond, ts,\n",
    "                                            clip_denoised=self.clip_denoised,\n",
    "                                            quantize_denoised=quantize_denoised, return_x0=True,\n",
    "                                            temperature=temperature[i], noise_dropout=noise_dropout,\n",
    "                                            score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n",
    "            if mask is not None:\n",
    "                assert x0 is not None\n",
    "                img_orig = self.q_sample(x0, ts)\n",
    "                img = img_orig * mask + (1. - mask) * img\n",
    "\n",
    "            if i % log_every_t == 0 or i == timesteps - 1:\n",
    "                intermediates.append(x0_partial)\n",
    "            if callback: callback(i)\n",
    "            if img_callback: img_callback(img, i)\n",
    "        return img, intermediates\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, cond, shape, return_intermediates=False,\n",
    "                      x_T=None, verbose=True, callback=None, timesteps=None, quantize_denoised=False,\n",
    "                      mask=None, x0=None, img_callback=None, start_T=None,\n",
    "                      log_every_t=None):\n",
    "\n",
    "        if not log_every_t:\n",
    "            log_every_t = self.log_every_t\n",
    "        device = self.betas.device\n",
    "        b = shape[0]\n",
    "        if x_T is None:\n",
    "            img = torch.randn(shape, device=device)\n",
    "        else:\n",
    "            img = x_T\n",
    "\n",
    "        intermediates = [img]\n",
    "        if timesteps is None:\n",
    "            timesteps = self.num_timesteps\n",
    "\n",
    "        if start_T is not None:\n",
    "            timesteps = min(timesteps, start_T)\n",
    "        iterator = tqdm(reversed(range(0, timesteps)), desc='Sampling t', total=timesteps) if verbose else reversed(\n",
    "            range(0, timesteps))\n",
    "\n",
    "        if mask is not None:\n",
    "            assert x0 is not None\n",
    "            assert x0.shape[2:3] == mask.shape[2:3]  # spatial size has to match\n",
    "\n",
    "        for i in iterator:\n",
    "            ts = torch.full((b,), i, device=device, dtype=torch.long)\n",
    "            if self.shorten_cond_schedule:\n",
    "                assert self.model.conditioning_key != 'hybrid'\n",
    "                tc = self.cond_ids[ts].to(cond.device)\n",
    "                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n",
    "\n",
    "            img = self.p_sample(img, cond, ts,\n",
    "                                clip_denoised=self.clip_denoised,\n",
    "                                quantize_denoised=quantize_denoised)\n",
    "            if mask is not None:\n",
    "                img_orig = self.q_sample(x0, ts)\n",
    "                img = img_orig * mask + (1. - mask) * img\n",
    "\n",
    "            if i % log_every_t == 0 or i == timesteps - 1:\n",
    "                intermediates.append(img)\n",
    "            if callback: callback(i)\n",
    "            if img_callback: img_callback(img, i)\n",
    "\n",
    "        if return_intermediates:\n",
    "            return img, intermediates\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, cond, batch_size=16, return_intermediates=False, x_T=None,\n",
    "               verbose=True, timesteps=None, quantize_denoised=False,\n",
    "               mask=None, x0=None, shape=None,**kwargs):\n",
    "        if shape is None:\n",
    "            shape = (batch_size, self.channels, self.image_size, self.image_size)\n",
    "        if cond is not None:\n",
    "            if isinstance(cond, dict):\n",
    "                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n",
    "                list(map(lambda x: x[:batch_size], cond[key])) for key in cond}\n",
    "            else:\n",
    "                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n",
    "        return self.p_sample_loop(cond,\n",
    "                                  shape,\n",
    "                                  return_intermediates=return_intermediates, x_T=x_T,\n",
    "                                  verbose=verbose, timesteps=timesteps, quantize_denoised=quantize_denoised,\n",
    "                                  mask=mask, x0=x0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_log(self, cond, batch_size, ddim, ddim_steps, **kwargs):\n",
    "        if ddim:\n",
    "            ddim_sampler = DDIMSampler(self)\n",
    "            if \"shape\" in kwargs:\n",
    "                shape = kwargs.pop(\"shape\")\n",
    "            else:\n",
    "                shape = (self.channels, self.image_size, self.image_size)\n",
    "            samples, intermediates = ddim_sampler.sample(ddim_steps, batch_size,\n",
    "                                                         shape, cond, verbose=False, **kwargs)\n",
    "\n",
    "        else:\n",
    "            samples, intermediates = self.sample(cond=cond, batch_size=batch_size,\n",
    "                                                 return_intermediates=True, **kwargs)\n",
    "\n",
    "        return samples, intermediates\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_unconditional_conditioning(self, batch_size, null_label=None):\n",
    "        if null_label is not None:\n",
    "            xc = null_label\n",
    "            if isinstance(xc, ListConfig):\n",
    "                xc = list(xc)\n",
    "            if isinstance(xc, dict) or isinstance(xc, list):\n",
    "                c = self.get_learned_conditioning(xc)\n",
    "            else:\n",
    "                if hasattr(xc, \"to\"):\n",
    "                    xc = xc.to(self.device)\n",
    "                c = self.get_learned_conditioning(xc)\n",
    "        else:\n",
    "            # todo: get null label from cond_stage_model\n",
    "            raise NotImplementedError()\n",
    "        c = repeat(c, '1 ... -> b ...', b=batch_size).to(self.device)\n",
    "        return c\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def log_images(self, batch, N=8, n_row=4, sample=True, ddim_steps=200, ddim_eta=1., return_keys=None,\n",
    "                   quantize_denoised=True, inpaint=True, plot_denoise_rows=False, plot_progressive_rows=True,\n",
    "                   plot_diffusion_rows=True, unconditional_guidance_scale=1., unconditional_guidance_label=None,\n",
    "                   use_ema_scope=True,\n",
    "                   **kwargs):\n",
    "        ema_scope = self.ema_scope if use_ema_scope else nullcontext\n",
    "        use_ddim = ddim_steps is not None\n",
    "\n",
    "        log = dict()\n",
    "        z, c, x, xrec, xc = self.get_input(batch, self.first_stage_key,\n",
    "                                           return_first_stage_outputs=True,\n",
    "                                           force_c_encode=True,\n",
    "                                           return_original_cond=True,\n",
    "                                           bs=N)\n",
    "        N = min(x.shape[0], N)\n",
    "        n_row = min(x.shape[0], n_row)\n",
    "        log[\"inputs\"] = x\n",
    "        log[\"reconstruction\"] = xrec\n",
    "        if self.model.conditioning_key is not None:\n",
    "            if hasattr(self.cond_stage_model, \"decode\"):\n",
    "                xc = self.cond_stage_model.decode(c)\n",
    "                log[\"conditioning\"] = xc\n",
    "            elif self.cond_stage_key in [\"caption\", \"txt\"]:\n",
    "                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[self.cond_stage_key], size=x.shape[2]//25)\n",
    "                log[\"conditioning\"] = xc\n",
    "            elif self.cond_stage_key == 'class_label':\n",
    "                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[\"human_label\"], size=x.shape[2]//25)\n",
    "                log['conditioning'] = xc\n",
    "            elif isimage(xc):\n",
    "                log[\"conditioning\"] = xc\n",
    "            if ismap(xc):\n",
    "                log[\"original_conditioning\"] = self.to_rgb(xc)\n",
    "\n",
    "        if plot_diffusion_rows:\n",
    "            # get diffusion row\n",
    "            diffusion_row = list()\n",
    "            z_start = z[:n_row]\n",
    "            for t in range(self.num_timesteps):\n",
    "                if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n",
    "                    t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n",
    "                    t = t.to(self.device).long()\n",
    "                    noise = torch.randn_like(z_start)\n",
    "                    z_noisy = self.q_sample(x_start=z_start, t=t, noise=noise)\n",
    "                    diffusion_row.append(self.decode_first_stage(z_noisy))\n",
    "\n",
    "            diffusion_row = torch.stack(diffusion_row)  # n_log_step, n_row, C, H, W\n",
    "            diffusion_grid = rearrange(diffusion_row, 'n b c h w -> b n c h w')\n",
    "            diffusion_grid = rearrange(diffusion_grid, 'b n c h w -> (b n) c h w')\n",
    "            diffusion_grid = make_grid(diffusion_grid, nrow=diffusion_row.shape[0])\n",
    "            log[\"diffusion_row\"] = diffusion_grid\n",
    "\n",
    "        if sample:\n",
    "            # get denoise row\n",
    "            with ema_scope(\"Sampling\"):\n",
    "                samples, z_denoise_row = self.sample_log(cond=c,batch_size=N,ddim=use_ddim,\n",
    "                                                         ddim_steps=ddim_steps,eta=ddim_eta)\n",
    "                # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True)\n",
    "            x_samples = self.decode_first_stage(samples)\n",
    "            log[\"samples\"] = x_samples\n",
    "            if plot_denoise_rows:\n",
    "                denoise_grid = self._get_denoise_row_from_list(z_denoise_row)\n",
    "                log[\"denoise_row\"] = denoise_grid\n",
    "\n",
    "            if quantize_denoised and not isinstance(self.first_stage_model, AutoencoderKL) and not isinstance(\n",
    "                    self.first_stage_model, IdentityFirstStage):\n",
    "                # also display when quantizing x0 while sampling\n",
    "                with ema_scope(\"Plotting Quantized Denoised\"):\n",
    "                    samples, z_denoise_row = self.sample_log(cond=c,batch_size=N,ddim=use_ddim,\n",
    "                                                             ddim_steps=ddim_steps,eta=ddim_eta,\n",
    "                                                             quantize_denoised=True)\n",
    "                    # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True,\n",
    "                    #                                      quantize_denoised=True)\n",
    "                x_samples = self.decode_first_stage(samples.to(self.device))\n",
    "                log[\"samples_x0_quantized\"] = x_samples\n",
    "\n",
    "        if unconditional_guidance_scale > 1.0:\n",
    "            uc = self.get_unconditional_conditioning(N, unconditional_guidance_label)\n",
    "            with ema_scope(\"Sampling with classifier-free guidance\"):\n",
    "                samples_cfg, _ = self.sample_log(cond=c, batch_size=N, ddim=use_ddim,\n",
    "                                                 ddim_steps=ddim_steps, eta=ddim_eta,\n",
    "                                                 unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                 unconditional_conditioning=uc,\n",
    "                                                 )\n",
    "                x_samples_cfg = self.decode_first_stage(samples_cfg)\n",
    "                log[f\"samples_cfg_scale_{unconditional_guidance_scale:.2f}\"] = x_samples_cfg\n",
    "\n",
    "        if inpaint:\n",
    "            # make a simple center square\n",
    "            b, h, w = z.shape[0], z.shape[2], z.shape[3]\n",
    "            mask = torch.ones(N, h, w).to(self.device)\n",
    "            # zeros will be filled in\n",
    "            mask[:, h // 4:3 * h // 4, w // 4:3 * w // 4] = 0.\n",
    "            mask = mask[:, None, ...]\n",
    "            with ema_scope(\"Plotting Inpaint\"):\n",
    "\n",
    "                samples, _ = self.sample_log(cond=c,batch_size=N,ddim=use_ddim, eta=ddim_eta,\n",
    "                                            ddim_steps=ddim_steps, x0=z[:N], mask=mask)\n",
    "            x_samples = self.decode_first_stage(samples.to(self.device))\n",
    "            log[\"samples_inpainting\"] = x_samples\n",
    "            log[\"mask\"] = mask\n",
    "\n",
    "            # outpaint\n",
    "            mask = 1. - mask\n",
    "            with ema_scope(\"Plotting Outpaint\"):\n",
    "                samples, _ = self.sample_log(cond=c, batch_size=N, ddim=use_ddim,eta=ddim_eta,\n",
    "                                            ddim_steps=ddim_steps, x0=z[:N], mask=mask)\n",
    "            x_samples = self.decode_first_stage(samples.to(self.device))\n",
    "            log[\"samples_outpainting\"] = x_samples\n",
    "\n",
    "        if plot_progressive_rows:\n",
    "            with ema_scope(\"Plotting Progressives\"):\n",
    "                img, progressives = self.progressive_denoising(c,\n",
    "                                                               shape=(self.channels, self.image_size, self.image_size),\n",
    "                                                               batch_size=N)\n",
    "            prog_row = self._get_denoise_row_from_list(progressives, desc=\"Progressive Generation\")\n",
    "            log[\"progressive_row\"] = prog_row\n",
    "\n",
    "        if return_keys:\n",
    "            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:\n",
    "                return log\n",
    "            else:\n",
    "                return {key: log[key] for key in return_keys}\n",
    "        return log\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = 1.0e-06\n",
    "        params = list(self.model.parameters())\n",
    "        if self.cond_stage_trainable:\n",
    "            print(f\"{self.__class__.__name__}: Also optimizing conditioner params!\")\n",
    "            params = params + list(self.cond_stage_model.parameters())\n",
    "        if self.learn_logvar:\n",
    "            print('Diffusion model optimizing logvar')\n",
    "            params.append(self.logvar)\n",
    "        opt = torch.optim.AdamW(params, lr=lr)\n",
    "        if self.use_scheduler:\n",
    "            assert 'target' in self.scheduler_config\n",
    "            scheduler = instantiate_from_config(self.scheduler_config)\n",
    "\n",
    "            print(\"Setting up LambdaLR scheduler...\")\n",
    "            scheduler = [\n",
    "                {\n",
    "                    'scheduler': LambdaLR(opt, lr_lambda=scheduler.schedule),\n",
    "                    'interval': 'step',\n",
    "                    'frequency': 1\n",
    "                }]\n",
    "            return [opt], scheduler\n",
    "        return opt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def to_rgb(self, x):\n",
    "        x = x.float()\n",
    "        if not hasattr(self, \"colorize\"):\n",
    "            self.colorize = torch.randn(3, x.shape[1], 1, 1).to(x)\n",
    "        x = nn.functional.conv2d(x, weight=self.colorize)\n",
    "        x = 2. * (x - x.min()) / (x.max() - x.min()) - 1.\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_encoder_input(self, batch, k,device, return_first_stage_outputs=False, force_c_encode=False,\n",
    "                  cond_key=None, return_original_cond=False, bs=None, return_x=False):\n",
    "        x = super().get_input(batch, k)\n",
    "        # print(\"ldmm\",x.shape)\n",
    "        if bs is not None:\n",
    "            x = x[:bs]\n",
    "        x = x.to(device)\n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encoder_forward(self,x,batch, device, return_first_stage_outputs=False, force_c_encode=False,\n",
    "                  cond_key=None, return_original_cond=False, bs=None, return_x=False):\n",
    "        ### Encoder\n",
    "        encoder_posterior = self.encode_first_stage(x)\n",
    "        z = self.get_first_stage_encoding(encoder_posterior).detach()\n",
    "        \n",
    "        if self.model.conditioning_key is not None:\n",
    "            if cond_key is None:\n",
    "                cond_key = self.cond_stage_key\n",
    "            if cond_key != self.first_stage_key:\n",
    "                if cond_key in ['caption', 'coordinates_bbox', \"txt\"]:\n",
    "                    xc = batch[cond_key]\n",
    "                elif cond_key == 'class_label':\n",
    "                    xc = batch\n",
    "                else:\n",
    "                    xc = super().get_input(batch, cond_key).to(device)\n",
    "            else:\n",
    "                xc = x\n",
    "            if not self.cond_stage_trainable or force_c_encode:\n",
    "                if isinstance(xc, dict) or isinstance(xc, list):\n",
    "                    c = self.get_learned_conditioning(xc)\n",
    "                else:\n",
    "                    c = self.get_learned_conditioning(xc.to(device))\n",
    "            else:\n",
    "                c = xc\n",
    "            if bs is not None:\n",
    "                c = c[:bs]\n",
    "\n",
    "            if self.use_positional_encodings:\n",
    "                pos_x, pos_y = self.compute_latent_shifts(batch)\n",
    "                ckey = __conditioning_keys__[self.model.conditioning_key]\n",
    "                c = {ckey: c, 'pos_x': pos_x, 'pos_y': pos_y}\n",
    "            if isinstance(c, dict):\n",
    "                for key in c:\n",
    "                    if key == \"class_label\":\n",
    "                        c[key] = c[key].to(device)\n",
    "        else:\n",
    "            c = None\n",
    "            xc = None\n",
    "            if self.use_positional_encodings:\n",
    "                pos_x, pos_y = self.compute_latent_shifts(batch)\n",
    "                c = {'pos_x': pos_x, 'pos_y': pos_y}\n",
    "        out = [z, c]\n",
    "\n",
    "        if return_first_stage_outputs:\n",
    "            xrec = self.decode_first_stage(z)\n",
    "            out.extend([x, xrec])\n",
    "        if return_x:\n",
    "            out.extend([x])\n",
    "        if return_original_cond:\n",
    "            out.append(xc)\n",
    "        return out\n",
    "    \n",
    "    def diffusion_forward(self, x, c, device, *args, **kwargs):\n",
    "        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=device).long()\n",
    "        if self.model.conditioning_key is not None:\n",
    "            assert c is not None\n",
    "            if self.cond_stage_trainable:\n",
    "                c = self.get_learned_conditioning(c)\n",
    "            if self.shorten_cond_schedule:  # TODO: drop this option\n",
    "                tc = self.cond_ids[t].to(device)\n",
    "                c = self.q_sample(x_start=c, t=tc, noise=torch.randn_like(c.float()))\n",
    "        return self.p_losses(x, c, t, device, *args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDiffusion(\n",
    "    linear_start=0.0015,\n",
    "    linear_end=0.0195,\n",
    "    num_timesteps_cond=1,\n",
    "    log_every_t=200,\n",
    "    timesteps=1000,\n",
    "    first_stage_key=\"image\",\n",
    "    cond_stage_key=\"class_label\",\n",
    "    image_size=32,\n",
    "    channels=4,\n",
    "    cond_stage_trainable=True,\n",
    "    conditioning_key=\"crossattn\",\n",
    "    monitor=\"val/loss_simple_ema\",\n",
    "    unet_config =  {\n",
    "        \"target\": \"ldm.modules.diffusionmodules.openaimodel.UNetModel\",\n",
    "        \"params\": {\n",
    "            \"image_size\": 32,\n",
    "            \"in_channels\": 4,\n",
    "            \"out_channels\": 4,\n",
    "            \"model_channels\": 256,\n",
    "            \"attention_resolutions\": [4, 2, 1],\n",
    "            \"num_res_blocks\": 2,\n",
    "            \"channel_mult\": [1, 2, 4],\n",
    "            \"num_head_channels\": 32,\n",
    "            \"use_spatial_transformer\": True,\n",
    "            \"transformer_depth\": 1,\n",
    "            \"context_dim\": 512\n",
    "        },\n",
    "    },\n",
    "    first_stage_config = {\n",
    "            \"target\": \"ldm.models.autoencoder.VQModelInterface\",\n",
    "            \"params\": {\n",
    "                \"embed_dim\": 4,\n",
    "                \"n_embed\": 256,\n",
    "                \"ckpt_path\": \"/home/hjhwang/stable-diffusion/configs/first_stage_models/vq-f8-model.ckpt\",\n",
    "                \"ddconfig\": {\n",
    "                    \"double_z\": False,\n",
    "                    \"z_channels\": 4,\n",
    "                    \"resolution\": 256,\n",
    "                    \"in_channels\": 3,\n",
    "                    \"out_ch\": 3,\n",
    "                    \"ch\": 128,\n",
    "                    \"ch_mult\": [1, 2, 2, 4],\n",
    "                    \"num_res_blocks\": 2,\n",
    "                    \"attn_resolutions\": [32],\n",
    "                    \"dropout\": 0.0\n",
    "                 },\n",
    "                \"lossconfig\": {\n",
    "                    \"target\": \"torch.nn.Identity\"\n",
    "                }\n",
    "            },\n",
    "            \"device\": device_1\n",
    "    },\n",
    "    cond_stage_config = {\n",
    "            \"target\": \"ldm.modules.encoders.modules.ClassEmbedder\",\n",
    "            \"params\": {\n",
    "                \"embed_dim\": 512,\n",
    "                \"key\": \"class_label\"\n",
    "            },\n",
    "            \"device\": device_1\n",
    "    }\n",
    ")\n",
    "optimizer = model.configure_optimizers()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train + Profiling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_epochs = 2\n",
    "use_scheduler = False\n",
    "row_list = []\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        if epoch == 0:\n",
    "            x = model.get_encoder_input(batch, model.first_stage_key, device_1)\n",
    "\n",
    "            # Encoder Forward\n",
    "            z, c = model.encoder_forward(x, batch, device_1)\n",
    "  \n",
    "            # Diffusion Forward\n",
    "            loss, loss_dict = model.diffusion_forward(z, c, device_1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Diffusion Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
    "        else:\n",
    "            iter_execution_data = {}\n",
    "            x = model.get_encoder_input(batch, model.first_stage_key, device_1)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            # Encoder Forward\n",
    "            start_time = time.time()\n",
    "            z, c = model.encoder_forward(x, batch, device_1)\n",
    "            torch.cuda.synchronize()\n",
    "            encoder_forward_time = time.time() - start_time\n",
    "\n",
    "            # Diffusion Forward\n",
    "            start_time = time.time()\n",
    "            loss, loss_dict = model.diffusion_forward(z, c, device_1)\n",
    "            torch.cuda.synchronize()\n",
    "            diffusion_forward_time = time.time() - start_time\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            # Diffusion Backward\n",
    "            start_time = time.time()\n",
    "            loss.backward()\n",
    "            torch.cuda.synchronize()\n",
    "            diffusion_backward_time = time.time() - start_time\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
    "            print(f'Encoder Forward Time: {encoder_forward_time:.6f}s, Diffusion Forward Time: {diffusion_forward_time:.6f}s, Diffusion Backward Time: {diffusion_backward_time:.6f}s')\n",
    "            iter_execution_data[\"iteration_number\"] = batch_idx+1\n",
    "            iter_execution_data[\"batch_size\"] = batch_size\n",
    "            iter_execution_data[\"encoder_forward\"] = encoder_forward_time\n",
    "            iter_execution_data[\"diffusion_forward\"] = diffusion_forward_time\n",
    "            iter_execution_data[\"diffusion_backward\"] = diffusion_backward_time\n",
    "            print(iter_execution_data)\n",
    "            row_list.append(iter_execution_data)\n",
    "            del iter_execution_data\n",
    "df = pd.DataFrame(row_list)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장\n",
    "csv_file_path = f\"execution_time_data_batch_size_{batch_size}.csv\"\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = pd.read_csv(\"/home/hjhwang/stable-diffusion/experiments/execution_time_data_batch_size_4.csv\",index_col=0)\n",
    "df_8 = pd.read_csv(\"/home/hjhwang/stable-diffusion/experiments/execution_time_data_batch_size_8.csv\",index_col=0)\n",
    "df_16 = pd.read_csv(\"/home/hjhwang/stable-diffusion/experiments/execution_time_data_batch_size_16.csv\",index_col=0)\n",
    "df_32= pd.read_csv(\"/home/hjhwang/stable-diffusion/experiments/execution_time_data_batch_size_32.csv\",index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df_4, df_8, df_16, df_32], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"stable_diffusion_train_execution_time_data.csv\"\n",
    "combined_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
